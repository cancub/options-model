{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from   tensorflow import keras\n",
    "from   tensorflow.keras import regularizers\n",
    "from   tensorflow.keras import Sequential\n",
    "from   tensorflow.keras.layers import Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   IPython import display\n",
    "from   matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local modules\n",
    "import config\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "TICKER     = 'SPY'\n",
    "EXPIRIES   = ['2020-08-07']\n",
    "MAX_MARGIN = 500\n",
    "MIN_PROFIT = 100\n",
    "DATA_SPLIT = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tensorboard\n",
    "LOGDIR = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(LOGDIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving the model\n",
    "PREFIX = 'model'\n",
    "TICKER_MODEL_DIR = os.path.join(config.ML_MODELS_DIR, TICKER)\n",
    "try:\n",
    "    os.mkdir(TICKER_MODEL_DIR)\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "    for tmpdir in glob.glob('{}/{}*'.format(TICKER_MODEL_DIR, PREFIX)):\n",
    "        shutil.rmtree(tmpdir)\n",
    "MODEL_DIR = tempfile.mkdtemp(prefix='model', dir=TICKER_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_df_list = []\n",
    "for exp in EXPIRIES:\n",
    "    print(exp)\n",
    "    data_df_list.append(\n",
    "        utils.sort_trades_df_columns(\n",
    "            utils.load_spreads(TICKER, exp, refresh=True, verbose=True)\n",
    "        )\n",
    "    )\n",
    "data_df = pd.concat(data_df_list, ignore_index=True)\n",
    "print('Loaded {} examples'.format(data_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normatlize all of the stuff that will be used for X.\n",
    "# NOTE: do this before removing examples based on open_margin.\n",
    "#       We want to include all data in the statistics.\n",
    "normalized_df, means, stds = utils.normalize_metadata_columns(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immediately save the metadata that we can\n",
    "means.to_pickle(os.path.join(MODEL_DIR, 'means'))\n",
    "stds.to_pickle(os.path.join(MODEL_DIR, 'stds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whittle the data down to only what we want to stomach in terms of\n",
    "# open margin\n",
    "viable_trades_df = normalized_df[normalized_df.open_margin <= MAX_MARGIN]\n",
    "\n",
    "# We don't need the open_margin anymore\n",
    "examples_df = viable_trades_df.drop(['open_margin'], axis=1)\n",
    "# examples_df = normalized_df.drop(['open_margin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop out the max_profit and compare it to our desired minimum profit\n",
    "labels = examples_df.pop('max_profit') >= MIN_PROFIT + utils.calculate_fee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the values to be used for working with the data\n",
    "BATCH_SIZE = 512\n",
    "BUFFER_SIZE = 100\n",
    "n_examples, n_features = examples_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((examples_df.values, labels.values)).shuffle(n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the data\n",
    "n_train = int(examples_df.shape[0] * DATA_SPLIT)\n",
    "train_dataset = dataset.take(n_train)\n",
    "test_dataset = dataset.skip(n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = n_train//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE).repeat()\n",
    "validate_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001,\n",
    "    decay_steps=STEPS_PER_EPOCH*5,\n",
    "    decay_rate=1,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "def get_optimizer():\n",
    "    return keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = os.path.join(MODEL_DIR, 'checkpoint')\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [\n",
    "        # tfdocs.modeling.EpochDots(),\n",
    "        model_checkpoint_callback,\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10),\n",
    "        tf.keras.callbacks.TensorBoard(LOGDIR/name),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, name, optimizer=None, max_epochs=200):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True, name='binary_crossentropy'),\n",
    "            'accuracy'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "        epochs=max_epochs,\n",
    "        validation_data=validate_dataset,\n",
    "        callbacks=get_callbacks(name),\n",
    "        verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(n_features,)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = compile_and_fit(model, 'sizes/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the remaining metadata\n",
    "history_dict = history.history\n",
    "best_epoch = history_dict['loss'].index(min(history['loss']))\n",
    "with open(os.path.join(MODEL_DIR, 'metadata'), 'w') as MF:\n",
    "    json.dump(\n",
    "        {\n",
    "            'ticker': TICKER,\n",
    "            'expiries': EXPIRIES,\n",
    "            'max_margin': MAX_MARGIN,\n",
    "            'min_profit': MIN_PROFIT,\n",
    "            'accuracy': history_dict['accuracy'][best_epoch],\n",
    "            'loss': history_dict['loss'][best_epoch],\n",
    "        },\n",
    "        MF\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tarball for this session (ignoring the directory when including filenames)\n",
    "files_to_tar = [os.path.basename(f) for f in glob.glob('{}/*'.format(MODEL_DIR))]\n",
    "tarball_path = os.path.join(config.ML_MODELS_DIR, TICKER, '{}.tar'.format(uuid.uuid4()))\n",
    "subprocess.check_call(['tar', '-C', MODEL_DIR, '-cf', tarball_path] + files_to_tar)\n",
    "shutil.rmtree(MODEL_DIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Open an embedded TensorBoard viewer\n",
    "%tensorboard --logdir {LOGDIR}/sizes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
