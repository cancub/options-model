{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from   tensorflow import keras\n",
    "from   tensorflow.keras import regularizers\n",
    "from   tensorflow.keras import Sequential\n",
    "from   tensorflow.keras.layers import Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local modules\n",
    "import config\n",
    "import datasets\n",
    "import op_stats\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "TICKER = 'SPY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving the model\n",
    "PREFIX = 'model'\n",
    "TICKER_MODEL_DIR = os.path.join(config.ML_MODELS_DIR, TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the values to be used for working with the data\n",
    "BATCH_SIZE     = 512\n",
    "BUFFER_SIZE    = 100\n",
    "MAX_MARGIN     = 10\n",
    "MIN_PROFIT     = 1\n",
    "MIN_DATAPOINTS = 4*10**6\n",
    "MAX_EPOCHS     = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(TICKER_MODEL_DIR)\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "    for tmpdir in glob.glob('{}/{}*'.format(TICKER_MODEL_DIR, PREFIX)):\n",
    "        shutil.rmtree(tmpdir)\n",
    "MODEL_DIR = tempfile.mkdtemp(prefix='model', dir=TICKER_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "ds = datasets.load_dataset(\n",
    "    TICKER,\n",
    "    max_margin=MAX_MARGIN,\n",
    "    min_profit=MIN_PROFIT,\n",
    "    total_datapoints=MIN_DATAPOINTS,\n",
    "    loss_ratio=3,\n",
    "    loss_pool_multiplier=2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "metadata = ds.metadata\n",
    "# Are we using the right ticker?\n",
    "assert(metadata['ticker'] == TICKER)\n",
    "# Make sure we're at least reasonably close to the desired number of datapoints\n",
    "assert(metadata['total_datapoints'] >= MIN_DATAPOINTS*0.99)\n",
    "# Make sure the remaining metadata is accurate\n",
    "assert(metadata['max_margin'] == MAX_MARGIN)\n",
    "assert(metadata['min_profit'] == MIN_PROFIT)\n",
    "\n",
    "STEPS_PER_EPOCH = ds.n_train//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the statistics\n",
    "pooled_means, pooled_variances = op_stats.pool_stats_from_stats_df(TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immediately save the metadata that we can\n",
    "pooled_means.to_pickle(\n",
    "    os.path.join(MODEL_DIR, 'means'),\n",
    "    protocol=config.PICKLE_PROTOCOL\n",
    ")\n",
    "pooled_variances.to_pickle(\n",
    "    os.path.join(MODEL_DIR, 'variances'),\n",
    "    protocol=config.PICKLE_PROTOCOL\n",
    ")\n",
    "pooled_stds = pooled_variances.pow(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It'll be important for code using this model to know how to order its\n",
    "# columns.\n",
    "feature_order = pooled_means.index.tolist()\n",
    "n_features = len(feature_order)\n",
    "print('{} features'.format(n_features))\n",
    "metadata['feature_order'] = feature_order\n",
    "print(feature_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and set up the datasets for train, validation and test\n",
    "(train_dataset,\n",
    " validate_dataset,\n",
    " test_dataset) = ds.get_sets(feature_order, pooled_means, pooled_stds)\n",
    "\n",
    "train_dataset    = train_dataset.shuffle(ds.n_train)\n",
    "train_dataset    = train_dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE).repeat()\n",
    "validate_dataset = validate_dataset.shuffle(ds.n_val).batch(BATCH_SIZE)\n",
    "test_dataset     = test_dataset.shuffle(ds.n_test).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.00001,\n",
    "    decay_steps=STEPS_PER_EPOCH,\n",
    "    decay_rate=1,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "checkpoint_filepath = os.path.join(MODEL_DIR, 'checkpoint')\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "class MetadataSaver(keras.callbacks.Callback):\n",
    "    _best_loss = np.inf\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['val_loss'] >= self._best_loss:\n",
    "            return\n",
    "        \n",
    "        self._best_loss = logs['val_loss']\n",
    "        with open(os.path.join(MODEL_DIR, 'metadata'), 'w') as MF:\n",
    "            metadata.update({\n",
    "                'accuracy': float(logs['val_accuracy']),\n",
    "                'loss': float(self._best_loss),\n",
    "            })\n",
    "            json.dump(metadata, MF)\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [\n",
    "        # tfdocs.modeling.EpochDots(),\n",
    "        model_checkpoint_callback,\n",
    "        MetadataSaver(),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "    ]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(n_features,)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr_schedule),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    validation_data=validate_dataset,\n",
    "                    callbacks=get_callbacks('testing'),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the testing data in numpy form\n",
    "test_Xs = []\n",
    "test_Ys = []\n",
    "for x, y in test_dataset.as_numpy_iterator():\n",
    "    test_Xs.append(x)\n",
    "    test_Ys.append(y)\n",
    "test_Xs = np.concatenate(test_Xs)\n",
    "test_Ys = np.concatenate(test_Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "preds = model.predict(test_Xs)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out what the percentiles are for the predictions\n",
    "percentiles = list(range(50, 100, 5)) + [99, 99.9, 99.99, 99.999]\n",
    "percentiles = dict(zip(percentiles, np.percentile(preds, percentiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Precision, Recall and F1 score for the predictions\n",
    "pred_trues = preds >= 0\n",
    "true_pos = pred_trues & test_Ys\n",
    "precision = true_pos.sum() / pred_trues.sum()\n",
    "recall = true_pos.sum() / test_Ys.sum()\n",
    "f1_score = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, 'metadata'), 'r+') as MF:\n",
    "    metadata = json.load(MF)\n",
    "    MF.seek(0)\n",
    "    metadata.update({\n",
    "        'percentiles': percentiles,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "    })\n",
    "    json.dump(metadata, MF)\n",
    "    MF.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tarball for this session (ignoring the directory when including filenames)\n",
    "files_to_tar = [os.path.basename(f) for f in glob.glob('{}/*'.format(MODEL_DIR))]\n",
    "tarball_path = os.path.join(config.ML_MODELS_DIR, TICKER, '{}.tar'.format(uuid.uuid4()))\n",
    "subprocess.check_call(['tar', '-C', MODEL_DIR, '-cf', tarball_path] + files_to_tar)\n",
    "shutil.rmtree(MODEL_DIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
