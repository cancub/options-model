{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from   tensorflow import keras\n",
    "from   tensorflow.keras import regularizers\n",
    "from   tensorflow.keras import Sequential\n",
    "from   tensorflow.keras.layers import Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   IPython import display\n",
    "from   matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER     = 'SPY'\n",
    "EXPIRIES   = ['2020-08-07', '2020-08-14', '2020-08-21']\n",
    "MAX_MARGIN = 500\n",
    "MIN_PROFIT = 100\n",
    "DATA_SPLIT = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-07\n",
      "Attempting to load saved spreads\n",
      "Loaded\n",
      "2020-08-14\n",
      "Attempting to load saved spreads\n",
      "Loaded\n",
      "2020-08-21\n",
      "Attempting to load saved spreads\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_df_list = []\n",
    "for exp in EXPIRIES:\n",
    "    print(exp)\n",
    "    data_df_list.append(utils.load_spreads(TICKER, exp, verbose=True))\n",
    "data_df = pd.concat(data_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whittle the data down to only what we want to stomach in terms of\n",
    "# open margin\n",
    "viable_trades_df = data_df[data_df.open_margin <= MAX_MARGIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normatlize all of the stuff that will be used for X.\n",
    "# NOTE: do this before removing examples based on open_margin.\n",
    "#       We want to include all data in the statistics.\n",
    "normalized_df = utils.normalize_metadata_columns(data_df)\n",
    "# normalized_df = utils.normalize_metadata_columns(viable_trades_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whittle the data down to only what we want to stomach in terms of\n",
    "# open margin\n",
    "viable_trades_df = normalized_df[normalized_df.open_margin <= MAX_MARGIN]\n",
    "\n",
    "# We don't need the open_margin anymore\n",
    "examples_df = viable_trades_df.drop(['open_margin'], axis=1)\n",
    "# examples_df = normalized_df.drop(['open_margin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_profit</th>\n",
       "      <th>minutes_to_expiry</th>\n",
       "      <th>leg1_type</th>\n",
       "      <th>leg1_strike</th>\n",
       "      <th>leg1_credit</th>\n",
       "      <th>leg1_volume</th>\n",
       "      <th>leg1_volatility</th>\n",
       "      <th>leg1_delta</th>\n",
       "      <th>leg1_gamma</th>\n",
       "      <th>leg1_theta</th>\n",
       "      <th>...</th>\n",
       "      <th>leg2_strike</th>\n",
       "      <th>leg2_credit</th>\n",
       "      <th>leg2_volume</th>\n",
       "      <th>leg2_volatility</th>\n",
       "      <th>leg2_delta</th>\n",
       "      <th>leg2_gamma</th>\n",
       "      <th>leg2_theta</th>\n",
       "      <th>leg2_vega</th>\n",
       "      <th>leg2_rho</th>\n",
       "      <th>leg2_openInterest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470033</td>\n",
       "      <td>-0.424913</td>\n",
       "      <td>-0.166593</td>\n",
       "      <td>-0.636639</td>\n",
       "      <td>1.171857</td>\n",
       "      <td>3.564627</td>\n",
       "      <td>-3.743056</td>\n",
       "      <td>0.609668</td>\n",
       "      <td>0.381299</td>\n",
       "      <td>-0.099722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522210</td>\n",
       "      <td>-0.517242</td>\n",
       "      <td>0.257368</td>\n",
       "      <td>-0.692987</td>\n",
       "      <td>0.913137</td>\n",
       "      <td>3.808895</td>\n",
       "      <td>-4.022905</td>\n",
       "      <td>0.684278</td>\n",
       "      <td>0.322560</td>\n",
       "      <td>0.024291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548298</td>\n",
       "      <td>-0.551682</td>\n",
       "      <td>0.679685</td>\n",
       "      <td>-0.720611</td>\n",
       "      <td>0.783524</td>\n",
       "      <td>3.697613</td>\n",
       "      <td>-3.914461</td>\n",
       "      <td>0.650294</td>\n",
       "      <td>0.292979</td>\n",
       "      <td>0.180380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>135.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574387</td>\n",
       "      <td>-0.577329</td>\n",
       "      <td>-0.024809</td>\n",
       "      <td>-0.737192</td>\n",
       "      <td>0.659739</td>\n",
       "      <td>3.442582</td>\n",
       "      <td>-3.652533</td>\n",
       "      <td>0.572390</td>\n",
       "      <td>0.264664</td>\n",
       "      <td>-0.200010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>168.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600475</td>\n",
       "      <td>-0.595648</td>\n",
       "      <td>-0.116760</td>\n",
       "      <td>-0.753330</td>\n",
       "      <td>0.545598</td>\n",
       "      <td>3.070564</td>\n",
       "      <td>-3.265441</td>\n",
       "      <td>0.458787</td>\n",
       "      <td>0.238528</td>\n",
       "      <td>0.524207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>164.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626564</td>\n",
       "      <td>-0.606640</td>\n",
       "      <td>-0.150783</td>\n",
       "      <td>-0.755815</td>\n",
       "      <td>0.443952</td>\n",
       "      <td>2.617790</td>\n",
       "      <td>-2.791465</td>\n",
       "      <td>0.320512</td>\n",
       "      <td>0.215215</td>\n",
       "      <td>0.013026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>157.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652652</td>\n",
       "      <td>-0.613235</td>\n",
       "      <td>-0.164949</td>\n",
       "      <td>-0.753429</td>\n",
       "      <td>0.356506</td>\n",
       "      <td>2.124139</td>\n",
       "      <td>-2.272804</td>\n",
       "      <td>0.169728</td>\n",
       "      <td>0.195124</td>\n",
       "      <td>-0.175141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>151.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678741</td>\n",
       "      <td>-0.617632</td>\n",
       "      <td>-0.163810</td>\n",
       "      <td>-0.764012</td>\n",
       "      <td>0.283801</td>\n",
       "      <td>1.627252</td>\n",
       "      <td>-1.749681</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>0.178411</td>\n",
       "      <td>-0.035949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>148.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704829</td>\n",
       "      <td>-0.619830</td>\n",
       "      <td>-0.206308</td>\n",
       "      <td>-0.769962</td>\n",
       "      <td>0.225370</td>\n",
       "      <td>1.158774</td>\n",
       "      <td>-1.255598</td>\n",
       "      <td>-0.125101</td>\n",
       "      <td>0.164965</td>\n",
       "      <td>-0.337913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>147.0</td>\n",
       "      <td>-1.345874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.46879</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>1.045069</td>\n",
       "      <td>3.785857</td>\n",
       "      <td>-3.987042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730917</td>\n",
       "      <td>-0.620563</td>\n",
       "      <td>-0.171020</td>\n",
       "      <td>-0.746426</td>\n",
       "      <td>0.179960</td>\n",
       "      <td>0.740996</td>\n",
       "      <td>-0.814550</td>\n",
       "      <td>-0.252694</td>\n",
       "      <td>0.154497</td>\n",
       "      <td>0.065437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_profit  minutes_to_expiry  leg1_type  leg1_strike  leg1_credit  \\\n",
       "2559        -7.0          -1.345874        1.0     0.509434      0.46879   \n",
       "2560        28.0          -1.345874        1.0     0.509434      0.46879   \n",
       "2561        77.0          -1.345874        1.0     0.509434      0.46879   \n",
       "2562       135.0          -1.345874        1.0     0.509434      0.46879   \n",
       "2563       168.0          -1.345874        1.0     0.509434      0.46879   \n",
       "2564       164.0          -1.345874        1.0     0.509434      0.46879   \n",
       "2565       157.0          -1.345874        1.0     0.509434      0.46879   \n",
       "2566       151.0          -1.345874        1.0     0.509434      0.46879   \n",
       "2567       148.0          -1.345874        1.0     0.509434      0.46879   \n",
       "2568       147.0          -1.345874        1.0     0.509434      0.46879   \n",
       "\n",
       "      leg1_volume  leg1_volatility  leg1_delta  leg1_gamma  leg1_theta  ...  \\\n",
       "2559    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "2560    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "2561    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "2562    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "2563    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "2564    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "2565    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "2566    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "2567    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "2568    -0.087808        -0.673049    1.045069    3.785857   -3.987042  ...   \n",
       "\n",
       "      leg2_strike  leg2_credit  leg2_volume  leg2_volatility  leg2_delta  \\\n",
       "2559     0.470033    -0.424913    -0.166593        -0.636639    1.171857   \n",
       "2560     0.522210    -0.517242     0.257368        -0.692987    0.913137   \n",
       "2561     0.548298    -0.551682     0.679685        -0.720611    0.783524   \n",
       "2562     0.574387    -0.577329    -0.024809        -0.737192    0.659739   \n",
       "2563     0.600475    -0.595648    -0.116760        -0.753330    0.545598   \n",
       "2564     0.626564    -0.606640    -0.150783        -0.755815    0.443952   \n",
       "2565     0.652652    -0.613235    -0.164949        -0.753429    0.356506   \n",
       "2566     0.678741    -0.617632    -0.163810        -0.764012    0.283801   \n",
       "2567     0.704829    -0.619830    -0.206308        -0.769962    0.225370   \n",
       "2568     0.730917    -0.620563    -0.171020        -0.746426    0.179960   \n",
       "\n",
       "      leg2_gamma  leg2_theta  leg2_vega  leg2_rho  leg2_openInterest  \n",
       "2559    3.564627   -3.743056   0.609668  0.381299          -0.099722  \n",
       "2560    3.808895   -4.022905   0.684278  0.322560           0.024291  \n",
       "2561    3.697613   -3.914461   0.650294  0.292979           0.180380  \n",
       "2562    3.442582   -3.652533   0.572390  0.264664          -0.200010  \n",
       "2563    3.070564   -3.265441   0.458787  0.238528           0.524207  \n",
       "2564    2.617790   -2.791465   0.320512  0.215215           0.013026  \n",
       "2565    2.124139   -2.272804   0.169728  0.195124          -0.175141  \n",
       "2566    1.627252   -1.749681   0.017989  0.178411          -0.035949  \n",
       "2567    1.158774   -1.255598  -0.125101  0.164965          -0.337913  \n",
       "2568    0.740996   -0.814550  -0.252694  0.154497           0.065437  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop out the max_profit and compare it to our desired minimum profit\n",
    "labels = examples_df.pop('max_profit') >= MIN_PROFIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the values to be used for working with the data\n",
    "BATCH_SIZE = 512\n",
    "BUFFER_SIZE = 100\n",
    "n_examples, n_features = examples_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((examples_df.values, labels.values)).shuffle(n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the data\n",
    "n_train = int(examples_df.shape[0] * DATA_SPLIT)\n",
    "train_dataset = dataset.take(n_train)\n",
    "test_dataset = dataset.skip(n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = n_train//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE).repeat()\n",
    "validate_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001,\n",
    "    decay_steps=STEPS_PER_EPOCH*5,\n",
    "    decay_rate=1,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "def get_optimizer():\n",
    "    return keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = os.path.join(config.ML_MODELS_DIR, 'checkpoint')\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [\n",
    "        # tfdocs.modeling.EpochDots(),\n",
    "        model_checkpoint_callback,\n",
    "#         tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10),\n",
    "        tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, name, optimizer=None, max_epochs=200):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True, name='binary_crossentropy'),\n",
    "            'accuracy'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "        epochs=max_epochs,\n",
    "        validation_data=validate_dataset,\n",
    "        callbacks=get_callbacks(name),\n",
    "        verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Dense(1024, activation='relu', input_shape=(n_features,)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.1),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(16, activation='relu'),\n",
    "#     Dense(1)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(n_features,)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               12288     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 162,689\n",
      "Trainable params: 162,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 1448 steps, validate for 621 steps\n",
      "Epoch 1/200\n",
      "   1/1448 [..............................] - ETA: 10:14:25 - loss: 0.6140 - binary_crossentropy: 0.6140 - accuracy: 0.9102WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.367245). Check your callbacks.\n",
      "   2/1448 [..............................] - ETA: 5:17:09 - loss: 0.5592 - binary_crossentropy: 0.5592 - accuracy: 0.9023 WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.208904). Check your callbacks.\n",
      "   3/1448 [..............................] - ETA: 3:32:08 - loss: 0.5123 - binary_crossentropy: 0.5123 - accuracy: 0.9030WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.104820). Check your callbacks.\n",
      "1448/1448 [==============================] - 82s 57ms/step - loss: 0.0790 - binary_crossentropy: 0.0790 - accuracy: 0.9649 - val_loss: 0.0531 - val_binary_crossentropy: 0.0531 - val_accuracy: 0.9791\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 55s 38ms/step - loss: 0.0493 - binary_crossentropy: 0.0493 - accuracy: 0.9787 - val_loss: 0.0457 - val_binary_crossentropy: 0.0457 - val_accuracy: 0.9817\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 54s 37ms/step - loss: 0.0426 - binary_crossentropy: 0.0426 - accuracy: 0.9816 - val_loss: 0.0374 - val_binary_crossentropy: 0.0374 - val_accuracy: 0.9823\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 51s 35ms/step - loss: 0.0378 - binary_crossentropy: 0.0378 - accuracy: 0.9836 - val_loss: 0.0378 - val_binary_crossentropy: 0.0378 - val_accuracy: 0.9824\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 51s 35ms/step - loss: 0.0342 - binary_crossentropy: 0.0342 - accuracy: 0.9851 - val_loss: 0.0305 - val_binary_crossentropy: 0.0305 - val_accuracy: 0.9860\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 50s 35ms/step - loss: 0.0313 - binary_crossentropy: 0.0313 - accuracy: 0.9864 - val_loss: 0.0317 - val_binary_crossentropy: 0.0317 - val_accuracy: 0.9854\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 51s 35ms/step - loss: 0.0293 - binary_crossentropy: 0.0293 - accuracy: 0.9872 - val_loss: 0.0285 - val_binary_crossentropy: 0.0285 - val_accuracy: 0.9865\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 54s 37ms/step - loss: 0.0274 - binary_crossentropy: 0.0274 - accuracy: 0.9880 - val_loss: 0.0253 - val_binary_crossentropy: 0.0253 - val_accuracy: 0.9883\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 85s 59ms/step - loss: 0.0256 - binary_crossentropy: 0.0256 - accuracy: 0.9888 - val_loss: 0.0247 - val_binary_crossentropy: 0.0247 - val_accuracy: 0.9888\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 85s 59ms/step - loss: 0.0249 - binary_crossentropy: 0.0249 - accuracy: 0.9892 - val_loss: 0.0216 - val_binary_crossentropy: 0.0216 - val_accuracy: 0.9904\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 50s 35ms/step - loss: 0.0235 - binary_crossentropy: 0.0235 - accuracy: 0.9896 - val_loss: 0.0246 - val_binary_crossentropy: 0.0246 - val_accuracy: 0.9899\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 47s 33ms/step - loss: 0.0222 - binary_crossentropy: 0.0222 - accuracy: 0.9900 - val_loss: 0.0227 - val_binary_crossentropy: 0.0227 - val_accuracy: 0.9883\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 52s 36ms/step - loss: 0.0215 - binary_crossentropy: 0.0215 - accuracy: 0.9905 - val_loss: 0.0190 - val_binary_crossentropy: 0.0190 - val_accuracy: 0.9921\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 55s 38ms/step - loss: 0.0202 - binary_crossentropy: 0.0202 - accuracy: 0.9912 - val_loss: 0.0200 - val_binary_crossentropy: 0.0200 - val_accuracy: 0.9903\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 52s 36ms/step - loss: 0.0194 - binary_crossentropy: 0.0194 - accuracy: 0.9913 - val_loss: 0.0194 - val_binary_crossentropy: 0.0194 - val_accuracy: 0.9909\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 53s 37ms/step - loss: 0.0186 - binary_crossentropy: 0.0186 - accuracy: 0.9916 - val_loss: 0.0191 - val_binary_crossentropy: 0.0191 - val_accuracy: 0.9911\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 52s 36ms/step - loss: 0.0178 - binary_crossentropy: 0.0178 - accuracy: 0.9920 - val_loss: 0.0158 - val_binary_crossentropy: 0.0158 - val_accuracy: 0.9931\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 49s 34ms/step - loss: 0.0172 - binary_crossentropy: 0.0172 - accuracy: 0.9922 - val_loss: 0.0158 - val_binary_crossentropy: 0.0158 - val_accuracy: 0.9928\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 56s 38ms/step - loss: 0.0167 - binary_crossentropy: 0.0167 - accuracy: 0.9925 - val_loss: 0.0176 - val_binary_crossentropy: 0.0176 - val_accuracy: 0.9927\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 54s 37ms/step - loss: 0.0161 - binary_crossentropy: 0.0161 - accuracy: 0.9927 - val_loss: 0.0138 - val_binary_crossentropy: 0.0138 - val_accuracy: 0.9936\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 53s 36ms/step - loss: 0.0154 - binary_crossentropy: 0.0154 - accuracy: 0.9930 - val_loss: 0.0143 - val_binary_crossentropy: 0.0143 - val_accuracy: 0.9934\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 53s 37ms/step - loss: 0.0150 - binary_crossentropy: 0.0150 - accuracy: 0.9933 - val_loss: 0.0152 - val_binary_crossentropy: 0.0152 - val_accuracy: 0.9940\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 54s 38ms/step - loss: 0.0145 - binary_crossentropy: 0.0145 - accuracy: 0.9936 - val_loss: 0.0147 - val_binary_crossentropy: 0.0147 - val_accuracy: 0.9941\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 79s 55ms/step - loss: 0.0143 - binary_crossentropy: 0.0143 - accuracy: 0.9936 - val_loss: 0.0127 - val_binary_crossentropy: 0.0127 - val_accuracy: 0.9940\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 100s 69ms/step - loss: 0.0157 - binary_crossentropy: 0.0157 - accuracy: 0.9933 - val_loss: 0.0129 - val_binary_crossentropy: 0.0129 - val_accuracy: 0.9946\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 79s 55ms/step - loss: 0.0134 - binary_crossentropy: 0.0134 - accuracy: 0.9940 - val_loss: 0.0130 - val_binary_crossentropy: 0.0130 - val_accuracy: 0.9949\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 55s 38ms/step - loss: 0.0129 - binary_crossentropy: 0.0129 - accuracy: 0.9942 - val_loss: 0.0116 - val_binary_crossentropy: 0.0116 - val_accuracy: 0.9951\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 77s 53ms/step - loss: 0.0126 - binary_crossentropy: 0.0126 - accuracy: 0.9945 - val_loss: 0.0120 - val_binary_crossentropy: 0.0120 - val_accuracy: 0.9951\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 64s 44ms/step - loss: 0.0125 - binary_crossentropy: 0.0125 - accuracy: 0.9944 - val_loss: 0.0112 - val_binary_crossentropy: 0.0112 - val_accuracy: 0.9954\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 56s 39ms/step - loss: 0.0122 - binary_crossentropy: 0.0122 - accuracy: 0.9946 - val_loss: 0.0104 - val_binary_crossentropy: 0.0104 - val_accuracy: 0.9955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 64s 44ms/step - loss: 0.0118 - binary_crossentropy: 0.0118 - accuracy: 0.9948 - val_loss: 0.0106 - val_binary_crossentropy: 0.0106 - val_accuracy: 0.9953\n",
      "Epoch 32/200\n",
      "1448/1448 [==============================] - 52s 36ms/step - loss: 0.0115 - binary_crossentropy: 0.0115 - accuracy: 0.9950 - val_loss: 0.0108 - val_binary_crossentropy: 0.0108 - val_accuracy: 0.9951\n",
      "Epoch 33/200\n",
      "1448/1448 [==============================] - 50s 35ms/step - loss: 0.0112 - binary_crossentropy: 0.0112 - accuracy: 0.9951 - val_loss: 0.0110 - val_binary_crossentropy: 0.0110 - val_accuracy: 0.9950\n",
      "Epoch 34/200\n",
      "1448/1448 [==============================] - 52s 36ms/step - loss: 0.0110 - binary_crossentropy: 0.0110 - accuracy: 0.9952 - val_loss: 0.0116 - val_binary_crossentropy: 0.0116 - val_accuracy: 0.9949\n",
      "Epoch 35/200\n",
      "1448/1448 [==============================] - 58s 40ms/step - loss: 0.0107 - binary_crossentropy: 0.0107 - accuracy: 0.9953 - val_loss: 0.0098 - val_binary_crossentropy: 0.0098 - val_accuracy: 0.9954\n",
      "Epoch 36/200\n",
      "1448/1448 [==============================] - 54s 37ms/step - loss: 0.0104 - binary_crossentropy: 0.0104 - accuracy: 0.9954 - val_loss: 0.0107 - val_binary_crossentropy: 0.0107 - val_accuracy: 0.9948\n",
      "Epoch 37/200\n",
      "1448/1448 [==============================] - 53s 37ms/step - loss: 0.0105 - binary_crossentropy: 0.0105 - accuracy: 0.9954 - val_loss: 0.0095 - val_binary_crossentropy: 0.0095 - val_accuracy: 0.9957\n",
      "Epoch 38/200\n",
      "1448/1448 [==============================] - 50s 35ms/step - loss: 0.0100 - binary_crossentropy: 0.0100 - accuracy: 0.9956 - val_loss: 0.0101 - val_binary_crossentropy: 0.0101 - val_accuracy: 0.9956\n",
      "Epoch 39/200\n",
      "1448/1448 [==============================] - 53s 37ms/step - loss: 0.0099 - binary_crossentropy: 0.0099 - accuracy: 0.9958 - val_loss: 0.0097 - val_binary_crossentropy: 0.0097 - val_accuracy: 0.9963\n",
      "Epoch 40/200\n",
      "1448/1448 [==============================] - 115s 79ms/step - loss: 0.0098 - binary_crossentropy: 0.0098 - accuracy: 0.9958 - val_loss: 0.0093 - val_binary_crossentropy: 0.0093 - val_accuracy: 0.9959\n",
      "Epoch 41/200\n",
      "1448/1448 [==============================] - 74s 51ms/step - loss: 0.0096 - binary_crossentropy: 0.0096 - accuracy: 0.9958 - val_loss: 0.0084 - val_binary_crossentropy: 0.0084 - val_accuracy: 0.9965\n",
      "Epoch 42/200\n",
      "1448/1448 [==============================] - 96s 66ms/step - loss: 0.0092 - binary_crossentropy: 0.0092 - accuracy: 0.9961 - val_loss: 0.0092 - val_binary_crossentropy: 0.0092 - val_accuracy: 0.9961\n",
      "Epoch 43/200\n",
      "1448/1448 [==============================] - 88s 61ms/step - loss: 0.0092 - binary_crossentropy: 0.0092 - accuracy: 0.9960 - val_loss: 0.0084 - val_binary_crossentropy: 0.0084 - val_accuracy: 0.9964\n",
      "Epoch 44/200\n",
      "1448/1448 [==============================] - 127s 88ms/step - loss: 0.0091 - binary_crossentropy: 0.0091 - accuracy: 0.9960 - val_loss: 0.0083 - val_binary_crossentropy: 0.0083 - val_accuracy: 0.9965\n",
      "Epoch 45/200\n",
      "1448/1448 [==============================] - 68s 47ms/step - loss: 0.0090 - binary_crossentropy: 0.0090 - accuracy: 0.9962 - val_loss: 0.0082 - val_binary_crossentropy: 0.0082 - val_accuracy: 0.9964\n",
      "Epoch 46/200\n",
      "1448/1448 [==============================] - 55s 38ms/step - loss: 0.0088 - binary_crossentropy: 0.0088 - accuracy: 0.9962 - val_loss: 0.0088 - val_binary_crossentropy: 0.0088 - val_accuracy: 0.9963\n",
      "Epoch 47/200\n",
      "1448/1448 [==============================] - 128s 88ms/step - loss: 0.0085 - binary_crossentropy: 0.0085 - accuracy: 0.9964 - val_loss: 0.0087 - val_binary_crossentropy: 0.0087 - val_accuracy: 0.9964\n",
      "Epoch 48/200\n",
      "1448/1448 [==============================] - 108s 74ms/step - loss: 0.0084 - binary_crossentropy: 0.0084 - accuracy: 0.9964 - val_loss: 0.0093 - val_binary_crossentropy: 0.0093 - val_accuracy: 0.9961\n",
      "Epoch 49/200\n",
      "1448/1448 [==============================] - 65s 45ms/step - loss: 0.0083 - binary_crossentropy: 0.0083 - accuracy: 0.9964 - val_loss: 0.0089 - val_binary_crossentropy: 0.0089 - val_accuracy: 0.9962\n",
      "Epoch 50/200\n",
      "1448/1448 [==============================] - 93s 64ms/step - loss: 0.0081 - binary_crossentropy: 0.0081 - accuracy: 0.9965 - val_loss: 0.0078 - val_binary_crossentropy: 0.0078 - val_accuracy: 0.9967\n",
      "Epoch 51/200\n",
      "1446/1448 [============================>.] - ETA: 0s - loss: 0.0079 - binary_crossentropy: 0.0079 - accuracy: 0.9966"
     ]
    }
   ],
   "source": [
    "size_histories['test'] = compile_and_fit(model, 'sizes/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Open an embedded TensorBoard viewer\n",
    "%tensorboard --logdir {logdir}/sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
